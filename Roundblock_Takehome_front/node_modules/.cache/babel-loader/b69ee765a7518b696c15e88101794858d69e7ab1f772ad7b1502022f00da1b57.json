{"ast":null,"code":"import { makeAutoObservable } from 'mobx';\nimport { history, setToken, getToken, removeToken } from '@/utils';\nimport * as openai from 'openai'; // 导入openai库\nimport { tokenCounter } from '../utils';\nclass ApiStore {\n  constructor() {\n    var _this = this;\n    this.apiKey = getToken() || '';\n    this.selectedModel = localStorage.getItem('selectedModel') || 'gpt-3.5-turbo';\n    this.setApiKey = apiKey => {\n      console.log(apiKey);\n      this.apiKey = apiKey;\n      setToken(apiKey);\n    };\n    this.getApiKey = () => {\n      return this.apiKey;\n    };\n    this.removeApiKey = () => {\n      this.apiKey = '';\n      removeToken();\n      history.push('/login');\n    };\n    this.setSelectedModel = model => {\n      this.selectedModel = model;\n      localStorage.setItem('selectedModel', model);\n    };\n    this.getSelectedModel = () => {\n      return this.selectedModel;\n    };\n    this.processStream = async () => {\n      const {\n        done,\n        value\n      } = await reader.read();\n      if (done) {\n        // Stream has ended; return the result\n        return result;\n      }\n      const text = new TextDecoder(\"utf-8\").decode(value);\n      const lines = text === null || text === void 0 ? void 0 : text.split(\"\\n\").filter(line => line.trim() !== \"\");\n      for (const line of lines) {\n        const message = line.replace(/^data: /, \"\");\n        if (message === \"[DONE]\") {\n          await reader.cancel(); // Stop reading the stream\n          break; // Stream finished\n        }\n\n        try {\n          const parsed = JSON.parse(message);\n          result += parsed.choices[0].text; // Append the choice text to the result\n        } catch (error) {\n          console.error(\"Could not JSON parse stream message\", message, error);\n        }\n      }\n      return processStream(); // Continue processing the stream\n    };\n    this.callEngines = async function () {\n      let prompt = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"test\";\n      let model = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"text-davinci-003\";\n      let maxTokens = prompt !== \"test\" ? 3700 - tokenCounter(prompt) : 1;\n      try {\n        const response = await fetch(`https://api.openai.com/v1/engines/${model}/completions`, {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n            Authorization: `Bearer ${_this.apiKey}`\n          },\n          body: JSON.stringify({\n            prompt: prompt,\n            max_tokens: 5,\n            stream: true\n          })\n        });\n        if (response.status === 200) {\n          const reader = response.body.getReader();\n          let result = \"\";\n          const finalResult = await _this.processStream();\n\n          // Return the test case result or the actual completion content\n          return prompt === \"test\" ? true : finalResult;\n        } else {\n          throw new Error(`Error: ${response.status} ${response.statusText}`);\n        }\n      } catch (error) {\n        console.error(error);\n        return false;\n      }\n    };\n    this.callGPT = async function () {\n      let prompt = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 'test';\n      let model = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"gpt-4\";\n      try {\n        let maxTokens;\n        if (prompt === 'test') {\n          maxTokens = 5;\n        } else if (model === \"gpt-4\") {\n          maxTokens = 7700 - tokenCounter(prompt);\n        } else {\n          maxTokens = 3700 - tokenCounter(prompt);\n        }\n        const response = await fetch(`https://api.openai.com/v1/chat/completions`, {\n          method: 'POST',\n          headers: {\n            'Content-Type': 'application/json',\n            'Authorization': `Bearer ${_this.apiKey}`\n          },\n          body: JSON.stringify({\n            model: model,\n            messages: [{\n              role: \"user\",\n              content: prompt\n            }],\n            max_tokens: maxTokens,\n            temperature: 0.5,\n            stream: true\n          })\n        });\n        console.log(response);\n        if (response.status === 200) {\n          const responseData = await response.json();\n          console.log(responseData);\n          return prompt === 'test' ? true : responseData.choices[0].message.content;\n        } else {\n          throw new Error(`Error: ${response.status} ${response.statusText}`);\n        }\n      } catch (error) {\n        console.error(error);\n        return false;\n      }\n    };\n    this.callModel = async (prompt, model) => {\n      if (model === 'gpt-3.5-turbo' || model === 'gpt-4') {\n        return await this.callGPT(prompt, model);\n      } else {\n        return await this.callEngines(prompt, model);\n      }\n    };\n    makeAutoObservable(this);\n  }\n}\nexport default ApiStore;","map":{"version":3,"names":["makeAutoObservable","history","setToken","getToken","removeToken","openai","tokenCounter","ApiStore","constructor","_this","apiKey","selectedModel","localStorage","getItem","setApiKey","console","log","getApiKey","removeApiKey","push","setSelectedModel","model","setItem","getSelectedModel","processStream","done","value","reader","read","result","text","TextDecoder","decode","lines","split","filter","line","trim","message","replace","cancel","parsed","JSON","parse","choices","error","callEngines","prompt","arguments","length","undefined","maxTokens","response","fetch","method","headers","Authorization","body","stringify","max_tokens","stream","status","getReader","finalResult","Error","statusText","callGPT","messages","role","content","temperature","responseData","json","callModel"],"sources":["C:/Users/94272/Desktop/Job/my-personal_website/beattather/src/store/api.Store.js"],"sourcesContent":["import { makeAutoObservable } from 'mobx';\r\nimport { history, setToken, getToken, removeToken } from '@/utils';\r\nimport * as openai from 'openai'; // 导入openai库\r\nimport { tokenCounter } from '../utils';\r\nclass ApiStore {\r\n  apiKey = getToken() || '';\r\n  selectedModel = localStorage.getItem('selectedModel') || 'gpt-3.5-turbo';\r\n\r\n  constructor() {\r\n    makeAutoObservable(this);\r\n  }\r\n\r\n  setApiKey = (apiKey) => {\r\n    console.log(apiKey);\r\n    this.apiKey = apiKey;\r\n    setToken(apiKey);\r\n  };\r\n\r\n  getApiKey = () => {\r\n    return this.apiKey;\r\n  };\r\n\r\n  removeApiKey = () => {\r\n    this.apiKey = '';\r\n    removeToken();\r\n    history.push('/login');\r\n  };\r\n\r\n  setSelectedModel = (model) => {\r\n    this.selectedModel = model;\r\n    localStorage.setItem('selectedModel', model);\r\n  };\r\n\r\n  getSelectedModel = () => {\r\n    return this.selectedModel;\r\n  };\r\n\r\n  processStream = async () => {\r\n    const { done, value } = await reader.read();\r\n\r\n    if (done) {\r\n      // Stream has ended; return the result\r\n      return result;\r\n    }\r\n\r\n    const text = new TextDecoder(\"utf-8\").decode(value);\r\n    const lines = text?.split(\"\\n\").filter((line) => line.trim() !== \"\");\r\n\r\n    for (const line of lines) {\r\n      const message = line.replace(/^data: /, \"\");\r\n      if (message === \"[DONE]\") {\r\n        await reader.cancel(); // Stop reading the stream\r\n        break; // Stream finished\r\n      }\r\n      try {\r\n        const parsed = JSON.parse(message);\r\n        result += parsed.choices[0].text; // Append the choice text to the result\r\n      } catch (error) {\r\n        console.error(\"Could not JSON parse stream message\", message, error);\r\n      }\r\n    }\r\n\r\n    return processStream(); // Continue processing the stream\r\n  };\r\n\r\n  callEngines = async (prompt = \"test\", model = \"text-davinci-003\") => {\r\n    let maxTokens = prompt !== \"test\" ? 3700 - tokenCounter(prompt) : 1;\r\n    try {\r\n      const response = await fetch(\r\n        `https://api.openai.com/v1/engines/${model}/completions`,\r\n        {\r\n          method: \"POST\",\r\n          headers: {\r\n            \"Content-Type\": \"application/json\",\r\n            Authorization: `Bearer ${this.apiKey}`,\r\n          },\r\n          body: JSON.stringify({\r\n            prompt: prompt,\r\n            max_tokens: 5,\r\n            stream: true,\r\n          }),\r\n        }\r\n      );\r\n  \r\n      if (response.status === 200) {\r\n        const reader = response.body.getReader();\r\n        let result = \"\";\r\n  \r\n        \r\n  \r\n        const finalResult = await this.processStream();\r\n        \r\n  \r\n        // Return the test case result or the actual completion content\r\n        return prompt === \"test\" ? true : finalResult;\r\n      } else {\r\n        throw new Error(`Error: ${response.status} ${response.statusText}`);\r\n      }\r\n    } catch (error) {\r\n      console.error(error);\r\n      return false;\r\n    }\r\n  };\r\n  \r\n  \r\n\r\n  callGPT = async (prompt = 'test', model = \"gpt-4\") => {\r\n    try {\r\n      let maxTokens;\r\n      if (prompt === 'test') {\r\n        maxTokens = 5;\r\n      } else if (model === \"gpt-4\") {\r\n        maxTokens = 7700 - tokenCounter(prompt);\r\n      } else {\r\n        maxTokens = 3700 - tokenCounter(prompt);\r\n      }\r\n  \r\n      const response = await fetch(`https://api.openai.com/v1/chat/completions`, {\r\n        method: 'POST',\r\n        headers: {\r\n          'Content-Type': 'application/json',\r\n          'Authorization': `Bearer ${this.apiKey}`,\r\n        },\r\n        body: JSON.stringify({\r\n          model: model,\r\n          messages: [{ role: \"user\", content: prompt }],\r\n          max_tokens: maxTokens,\r\n          temperature: 0.5,\r\n          stream: true\r\n        }),\r\n      });\r\n      console.log(response);\r\n      if (response.status === 200) {\r\n        const responseData = await response.json();\r\n        console.log(responseData);\r\n        return prompt === 'test' ? true : responseData.choices[0].message.content;\r\n      } else {\r\n        throw new Error(`Error: ${response.status} ${response.statusText}`);\r\n      }\r\n    } catch (error) {\r\n      console.error(error);\r\n      return false;\r\n    }\r\n  };\r\n  \r\n\r\n  callModel = async (prompt , model ) => {\r\n    if (model === 'gpt-3.5-turbo' || model === 'gpt-4') {\r\n      return await this.callGPT(prompt, model);\r\n    } else {\r\n      return await this.callEngines(prompt, model);\r\n    }\r\n  };\r\n\r\n  \r\n}\r\n\r\nexport default ApiStore;"],"mappings":"AAAA,SAASA,kBAAkB,QAAQ,MAAM;AACzC,SAASC,OAAO,EAAEC,QAAQ,EAAEC,QAAQ,EAAEC,WAAW,QAAQ,SAAS;AAClE,OAAO,KAAKC,MAAM,MAAM,QAAQ,CAAC,CAAC;AAClC,SAASC,YAAY,QAAQ,UAAU;AACvC,MAAMC,QAAQ,CAAC;EAIbC,WAAWA,CAAA,EAAG;IAAA,IAAAC,KAAA;IAAA,KAHdC,MAAM,GAAGP,QAAQ,EAAE,IAAI,EAAE;IAAA,KACzBQ,aAAa,GAAGC,YAAY,CAACC,OAAO,CAAC,eAAe,CAAC,IAAI,eAAe;IAAA,KAMxEC,SAAS,GAAIJ,MAAM,IAAK;MACtBK,OAAO,CAACC,GAAG,CAACN,MAAM,CAAC;MACnB,IAAI,CAACA,MAAM,GAAGA,MAAM;MACpBR,QAAQ,CAACQ,MAAM,CAAC;IAClB,CAAC;IAAA,KAEDO,SAAS,GAAG,MAAM;MAChB,OAAO,IAAI,CAACP,MAAM;IACpB,CAAC;IAAA,KAEDQ,YAAY,GAAG,MAAM;MACnB,IAAI,CAACR,MAAM,GAAG,EAAE;MAChBN,WAAW,EAAE;MACbH,OAAO,CAACkB,IAAI,CAAC,QAAQ,CAAC;IACxB,CAAC;IAAA,KAEDC,gBAAgB,GAAIC,KAAK,IAAK;MAC5B,IAAI,CAACV,aAAa,GAAGU,KAAK;MAC1BT,YAAY,CAACU,OAAO,CAAC,eAAe,EAAED,KAAK,CAAC;IAC9C,CAAC;IAAA,KAEDE,gBAAgB,GAAG,MAAM;MACvB,OAAO,IAAI,CAACZ,aAAa;IAC3B,CAAC;IAAA,KAEDa,aAAa,GAAG,YAAY;MAC1B,MAAM;QAAEC,IAAI;QAAEC;MAAM,CAAC,GAAG,MAAMC,MAAM,CAACC,IAAI,EAAE;MAE3C,IAAIH,IAAI,EAAE;QACR;QACA,OAAOI,MAAM;MACf;MAEA,MAAMC,IAAI,GAAG,IAAIC,WAAW,CAAC,OAAO,CAAC,CAACC,MAAM,CAACN,KAAK,CAAC;MACnD,MAAMO,KAAK,GAAGH,IAAI,aAAJA,IAAI,uBAAJA,IAAI,CAAEI,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAAEC,IAAI,IAAKA,IAAI,CAACC,IAAI,EAAE,KAAK,EAAE,CAAC;MAEpE,KAAK,MAAMD,IAAI,IAAIH,KAAK,EAAE;QACxB,MAAMK,OAAO,GAAGF,IAAI,CAACG,OAAO,CAAC,SAAS,EAAE,EAAE,CAAC;QAC3C,IAAID,OAAO,KAAK,QAAQ,EAAE;UACxB,MAAMX,MAAM,CAACa,MAAM,EAAE,CAAC,CAAC;UACvB,MAAM,CAAC;QACT;;QACA,IAAI;UACF,MAAMC,MAAM,GAAGC,IAAI,CAACC,KAAK,CAACL,OAAO,CAAC;UAClCT,MAAM,IAAIY,MAAM,CAACG,OAAO,CAAC,CAAC,CAAC,CAACd,IAAI,CAAC,CAAC;QACpC,CAAC,CAAC,OAAOe,KAAK,EAAE;UACd9B,OAAO,CAAC8B,KAAK,CAAC,qCAAqC,EAAEP,OAAO,EAAEO,KAAK,CAAC;QACtE;MACF;MAEA,OAAOrB,aAAa,EAAE,CAAC,CAAC;IAC1B,CAAC;IAAA,KAEDsB,WAAW,GAAG,kBAAuD;MAAA,IAAhDC,MAAM,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,MAAM;MAAA,IAAE3B,KAAK,GAAA2B,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,kBAAkB;MAC9D,IAAIG,SAAS,GAAGJ,MAAM,KAAK,MAAM,GAAG,IAAI,GAAGzC,YAAY,CAACyC,MAAM,CAAC,GAAG,CAAC;MACnE,IAAI;QACF,MAAMK,QAAQ,GAAG,MAAMC,KAAK,CACzB,qCAAoChC,KAAM,cAAa,EACxD;UACEiC,MAAM,EAAE,MAAM;UACdC,OAAO,EAAE;YACP,cAAc,EAAE,kBAAkB;YAClCC,aAAa,EAAG,UAAS/C,KAAI,CAACC,MAAO;UACvC,CAAC;UACD+C,IAAI,EAAEf,IAAI,CAACgB,SAAS,CAAC;YACnBX,MAAM,EAAEA,MAAM;YACdY,UAAU,EAAE,CAAC;YACbC,MAAM,EAAE;UACV,CAAC;QACH,CAAC,CACF;QAED,IAAIR,QAAQ,CAACS,MAAM,KAAK,GAAG,EAAE;UAC3B,MAAMlC,MAAM,GAAGyB,QAAQ,CAACK,IAAI,CAACK,SAAS,EAAE;UACxC,IAAIjC,MAAM,GAAG,EAAE;UAIf,MAAMkC,WAAW,GAAG,MAAMtD,KAAI,CAACe,aAAa,EAAE;;UAG9C;UACA,OAAOuB,MAAM,KAAK,MAAM,GAAG,IAAI,GAAGgB,WAAW;QAC/C,CAAC,MAAM;UACL,MAAM,IAAIC,KAAK,CAAE,UAASZ,QAAQ,CAACS,MAAO,IAAGT,QAAQ,CAACa,UAAW,EAAC,CAAC;QACrE;MACF,CAAC,CAAC,OAAOpB,KAAK,EAAE;QACd9B,OAAO,CAAC8B,KAAK,CAACA,KAAK,CAAC;QACpB,OAAO,KAAK;MACd;IACF,CAAC;IAAA,KAIDqB,OAAO,GAAG,kBAA4C;MAAA,IAArCnB,MAAM,GAAAC,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,MAAM;MAAA,IAAE3B,KAAK,GAAA2B,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAE,SAAA,GAAAF,SAAA,MAAG,OAAO;MAC/C,IAAI;QACF,IAAIG,SAAS;QACb,IAAIJ,MAAM,KAAK,MAAM,EAAE;UACrBI,SAAS,GAAG,CAAC;QACf,CAAC,MAAM,IAAI9B,KAAK,KAAK,OAAO,EAAE;UAC5B8B,SAAS,GAAG,IAAI,GAAG7C,YAAY,CAACyC,MAAM,CAAC;QACzC,CAAC,MAAM;UACLI,SAAS,GAAG,IAAI,GAAG7C,YAAY,CAACyC,MAAM,CAAC;QACzC;QAEA,MAAMK,QAAQ,GAAG,MAAMC,KAAK,CAAE,4CAA2C,EAAE;UACzEC,MAAM,EAAE,MAAM;UACdC,OAAO,EAAE;YACP,cAAc,EAAE,kBAAkB;YAClC,eAAe,EAAG,UAAS9C,KAAI,CAACC,MAAO;UACzC,CAAC;UACD+C,IAAI,EAAEf,IAAI,CAACgB,SAAS,CAAC;YACnBrC,KAAK,EAAEA,KAAK;YACZ8C,QAAQ,EAAE,CAAC;cAAEC,IAAI,EAAE,MAAM;cAAEC,OAAO,EAAEtB;YAAO,CAAC,CAAC;YAC7CY,UAAU,EAAER,SAAS;YACrBmB,WAAW,EAAE,GAAG;YAChBV,MAAM,EAAE;UACV,CAAC;QACH,CAAC,CAAC;QACF7C,OAAO,CAACC,GAAG,CAACoC,QAAQ,CAAC;QACrB,IAAIA,QAAQ,CAACS,MAAM,KAAK,GAAG,EAAE;UAC3B,MAAMU,YAAY,GAAG,MAAMnB,QAAQ,CAACoB,IAAI,EAAE;UAC1CzD,OAAO,CAACC,GAAG,CAACuD,YAAY,CAAC;UACzB,OAAOxB,MAAM,KAAK,MAAM,GAAG,IAAI,GAAGwB,YAAY,CAAC3B,OAAO,CAAC,CAAC,CAAC,CAACN,OAAO,CAAC+B,OAAO;QAC3E,CAAC,MAAM;UACL,MAAM,IAAIL,KAAK,CAAE,UAASZ,QAAQ,CAACS,MAAO,IAAGT,QAAQ,CAACa,UAAW,EAAC,CAAC;QACrE;MACF,CAAC,CAAC,OAAOpB,KAAK,EAAE;QACd9B,OAAO,CAAC8B,KAAK,CAACA,KAAK,CAAC;QACpB,OAAO,KAAK;MACd;IACF,CAAC;IAAA,KAGD4B,SAAS,GAAG,OAAO1B,MAAM,EAAG1B,KAAK,KAAM;MACrC,IAAIA,KAAK,KAAK,eAAe,IAAIA,KAAK,KAAK,OAAO,EAAE;QAClD,OAAO,MAAM,IAAI,CAAC6C,OAAO,CAACnB,MAAM,EAAE1B,KAAK,CAAC;MAC1C,CAAC,MAAM;QACL,OAAO,MAAM,IAAI,CAACyB,WAAW,CAACC,MAAM,EAAE1B,KAAK,CAAC;MAC9C;IACF,CAAC;IA/ICrB,kBAAkB,CAAC,IAAI,CAAC;EAC1B;AAiJF;AAEA,eAAeO,QAAQ"},"metadata":{},"sourceType":"module","externalDependencies":[]}