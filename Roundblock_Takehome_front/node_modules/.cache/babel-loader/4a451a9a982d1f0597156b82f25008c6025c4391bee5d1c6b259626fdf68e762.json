{"ast":null,"code":"import { makeAutoObservable } from 'mobx';\nimport { history, setToken, getToken, removeToken } from '@/utils';\nimport * as openai from 'openai'; // 导入openai库\nimport { tokenCounter } from '../utils';\nimport { max } from 'lodash';\nclass ApiStore {\n  constructor() {\n    var _this = this;\n    this.apiKey = getToken() || '';\n    this.selectedModel = localStorage.getItem('selectedModel') || 'gpt-3.5-turbo';\n    this.setApiKey = apiKey => {\n      this.apiKey = apiKey;\n      setToken(apiKey);\n    };\n    this.getApiKey = () => {\n      return this.apiKey;\n    };\n    this.removeApiKey = () => {\n      this.apiKey = '';\n      removeToken();\n      history.push('/login');\n    };\n    this.setSelectedModel = model => {\n      this.selectedModel = model;\n      localStorage.setItem('selectedModel', model);\n    };\n    this.getSelectedModel = () => {\n      return this.selectedModel;\n    };\n    this.streamRequest = async (endpoint, prompt, model, maxTokens, updateCallback) => {\n      try {\n        const requestBody = {\n          model: model,\n          max_tokens: maxTokens,\n          temperature: 0.5,\n          stream: true\n        };\n        if (model === false) {\n          delete requestBody.model;\n          requestBody.prompt = prompt;\n        } else {\n          requestBody.messages = prompt;\n        }\n        const response = await fetch(endpoint, {\n          method: \"POST\",\n          headers: {\n            \"Content-Type\": \"application/json\",\n            Authorization: `Bearer ${this.apiKey}`\n          },\n          body: JSON.stringify(requestBody)\n        });\n        if (response.status === 200) {\n          if (prompt[0].content === \"test\") {\n            re;\n          }\n          const reader = response.body.getReader();\n          let result = \"\";\n          const processStream = async () => {\n            const {\n              done,\n              value\n            } = await reader.read();\n            if (done) {\n              // Stream has ended; return the result\n              return result;\n            }\n            const text = new TextDecoder(\"utf-8\").decode(value);\n            const lines = text === null || text === void 0 ? void 0 : text.split(\"\\n\").filter(line => line.trim() !== \"\");\n            for (const line of lines) {\n              const message = line.replace(/^data: /, \"\");\n              if (message === \"[DONE]\") {\n                await reader.cancel(); // Stop reading the stream\n                break; // Stream finished\n              }\n\n              try {\n                const parsed = JSON.parse(message);\n                let newText;\n                if (model === false) {\n                  newText = parsed.choices[0].text;\n                } else {\n                  newText = parsed.choices[0].delta.content;\n                }\n                if (newText !== undefined) {\n                  result += newText;\n                  if (updateCallback) {\n                    updateCallback(newText);\n                  }\n                }\n                // Append the choice text to the result\n\n                // Invoke the update callback with the new text\n              } catch (error) {\n                console.error(\"Could not JSON parse stream message\", message, error);\n              }\n            }\n            return processStream(); // Continue processing the stream\n          };\n\n          const finalResult = await processStream();\n          return prompt === \"test\" ? true : finalResult;\n        } else {\n          throw new Error(`Error: ${response.status} ${response.statusText}`);\n        }\n      } catch (error) {\n        console.error(error);\n        return false;\n      }\n    };\n    this.callEngines = async function () {\n      let prompt = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : \"test\";\n      let model = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"text-davinci-003\";\n      let updateCallback = arguments.length > 2 ? arguments[2] : undefined;\n      console.log(\"in callEngines prompt\", prompt);\n      const maxTokens = prompt !== \"test\" ? 3700 - tokenCounter(prompt) : 1;\n      const endpoint = `https://api.openai.com/v1/engines/${model}/completions`;\n      return await _this.streamRequest(endpoint, prompt, false, maxTokens, updateCallback);\n    };\n    this.callGPT = async function () {\n      let prompt = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : [{\n        \"role\": \"user\",\n        \"content\": 'test'\n      }];\n      let model = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : \"gpt-4\";\n      let updateCallback = arguments.length > 2 ? arguments[2] : undefined;\n      console.log(\"in callGPT prompt\", prompt);\n      let maxTokens;\n      if (prompt === \"test\") {\n        maxTokens = 5;\n      } else if (model === \"gpt-4\") {\n        maxTokens = 7700 - tokenCounter(prompt);\n      } else {\n        maxTokens = 3700 - tokenCounter(prompt);\n      }\n      const endpoint = 'https://api.openai.com/v1/chat/completions';\n      return await _this.streamRequest(endpoint, prompt, model, maxTokens, updateCallback);\n    };\n    this.callModel = async function () {\n      let prompt = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : [{\n        \"role\": \"user\",\n        \"content\": 'test'\n      }];\n      let model = arguments.length > 1 ? arguments[1] : undefined;\n      let updateCallback = arguments.length > 2 ? arguments[2] : undefined;\n      //console.log(\"updateCallback\",updateCallback);\n      console.log(\"in callModel prompt\", prompt);\n      if (model === 'gpt-3.5-turbo' || model === 'gpt-4') {\n        return await _this.callGPT(prompt, model, updateCallback);\n      } else {\n        return await _this.callEngines(prompt, model, updateCallback);\n      }\n    };\n    makeAutoObservable(this);\n  }\n}\nexport default ApiStore;","map":{"version":3,"names":["makeAutoObservable","history","setToken","getToken","removeToken","openai","tokenCounter","max","ApiStore","constructor","_this","apiKey","selectedModel","localStorage","getItem","setApiKey","getApiKey","removeApiKey","push","setSelectedModel","model","setItem","getSelectedModel","streamRequest","endpoint","prompt","maxTokens","updateCallback","requestBody","max_tokens","temperature","stream","messages","response","fetch","method","headers","Authorization","body","JSON","stringify","status","content","re","reader","getReader","result","processStream","done","value","read","text","TextDecoder","decode","lines","split","filter","line","trim","message","replace","cancel","parsed","parse","newText","choices","delta","undefined","error","console","finalResult","Error","statusText","callEngines","arguments","length","log","callGPT","callModel"],"sources":["C:/Users/94272/Desktop/Job/my-personal_website/beattather/src/store/api.Store.js"],"sourcesContent":["import { makeAutoObservable } from 'mobx';\r\nimport { history, setToken, getToken, removeToken } from '@/utils';\r\nimport * as openai from 'openai'; // 导入openai库\r\nimport { tokenCounter } from '../utils';\r\nimport { max } from 'lodash';\r\nclass ApiStore {\r\n  apiKey = getToken() || '';\r\n  selectedModel = localStorage.getItem('selectedModel') || 'gpt-3.5-turbo';\r\n\r\n  constructor() {\r\n    makeAutoObservable(this);\r\n  }\r\n\r\n  setApiKey = (apiKey) => {\r\n    \r\n    this.apiKey = apiKey;\r\n    setToken(apiKey);\r\n  };\r\n\r\n  getApiKey = () => {\r\n    return this.apiKey;\r\n  };\r\n\r\n  removeApiKey = () => {\r\n    this.apiKey = '';\r\n    removeToken();\r\n    history.push('/login');\r\n  };\r\n\r\n  setSelectedModel = (model) => {\r\n    this.selectedModel = model;\r\n    localStorage.setItem('selectedModel', model);\r\n  };\r\n\r\n  getSelectedModel = () => {\r\n    return this.selectedModel;\r\n  };\r\n\r\n  \r\n  streamRequest = async (endpoint, prompt, model, maxTokens, updateCallback) => {\r\n    try {\r\n\r\n      const requestBody = {\r\n        model: model,\r\n\r\n        max_tokens: maxTokens,\r\n        temperature: 0.5,\r\n        stream: true,\r\n      };\r\n      \r\n      \r\n      if (model === false) {\r\n        delete requestBody.model;\r\n        requestBody.prompt = prompt;\r\n      }else{\r\n        requestBody.messages = prompt;\r\n      }\r\n      \r\n      const response = await fetch(endpoint, {\r\n        method: \"POST\",\r\n        headers: {\r\n          \"Content-Type\": \"application/json\",\r\n          Authorization: `Bearer ${this.apiKey}`,\r\n        },\r\n        body: JSON.stringify(requestBody),\r\n      });\r\n      \r\n  \r\n      if (response.status === 200) {\r\n        if (prompt[0].content === \"test\"){\r\n          re\r\n        }\r\n        const reader = response.body.getReader();\r\n        let result = \"\";\r\n  \r\n        const processStream = async () => {\r\n          const { done, value } = await reader.read();\r\n          \r\n          if (done) {\r\n            // Stream has ended; return the result\r\n            return result;\r\n          }\r\n    \r\n          const text = new TextDecoder(\"utf-8\").decode(value);\r\n          const lines = text?.split(\"\\n\").filter((line) => line.trim() !== \"\");\r\n          \r\n          for (const line of lines) {\r\n            \r\n            const message = line.replace(/^data: /, \"\");\r\n            if (message === \"[DONE]\") {\r\n              await reader.cancel(); // Stop reading the stream\r\n              break; // Stream finished\r\n            }\r\n            try {\r\n              const parsed = JSON.parse(message);\r\n              let newText;\r\n              if (model === false) {\r\n                newText = parsed.choices[0].text;\r\n              } else {\r\n                newText = parsed.choices[0].delta.content;\r\n              }\r\n                \r\n              if (newText !== undefined) {\r\n                result += newText;\r\n                if (updateCallback) {\r\n                  updateCallback(newText);\r\n                  \r\n                }\r\n              }\r\n               // Append the choice text to the result\r\n              \r\n              // Invoke the update callback with the new text\r\n              \r\n              \r\n            } catch (error) {\r\n              console.error(\"Could not JSON parse stream message\", message, error);\r\n            }\r\n          }\r\n    \r\n          return processStream(); // Continue processing the stream\r\n        };\r\n  \r\n        const finalResult = await processStream();\r\n        return prompt === \"test\" ? true : finalResult;\r\n      } else {\r\n        throw new Error(`Error: ${response.status} ${response.statusText}`);\r\n      }\r\n    } catch (error) {\r\n      console.error(error);\r\n      return false;\r\n    }\r\n  };\r\n  \r\n  callEngines = async (prompt = \"test\", model = \"text-davinci-003\", updateCallback) => {\r\n    console.log(\"in callEngines prompt\",prompt);\r\n    const maxTokens = prompt !== \"test\" ? 3700 - tokenCounter(prompt) : 1;\r\n    const endpoint = `https://api.openai.com/v1/engines/${model}/completions`;\r\n    \r\n    return await this.streamRequest(endpoint, prompt, false, maxTokens, updateCallback);\r\n  };\r\n  \r\n\r\n  callGPT = async (prompt = [{\"role\": \"user\", \"content\": 'test'}], model = \"gpt-4\", updateCallback) => {\r\n    console.log(\"in callGPT prompt\",prompt);\r\n    let maxTokens;\r\n    if (prompt === \"test\") {\r\n      maxTokens = 5;\r\n    } else if (model === \"gpt-4\") {\r\n      maxTokens = 7700 - tokenCounter(prompt);\r\n    } else {\r\n      maxTokens = 3700 - tokenCounter(prompt);\r\n    }\r\n    const endpoint = 'https://api.openai.com/v1/chat/completions';\r\n    \r\n    return await this.streamRequest(endpoint, prompt, model, maxTokens, updateCallback);\r\n  };\r\n  \r\n  \r\n\r\n  callModel = async (prompt = [{\"role\": \"user\", \"content\": 'test'}], model, updateCallback) => {\r\n    //console.log(\"updateCallback\",updateCallback);\r\n    console.log(\"in callModel prompt\",prompt);\r\n    if (model === 'gpt-3.5-turbo' || model === 'gpt-4') {\r\n      return await this.callGPT(prompt, model, updateCallback);\r\n    } else {\r\n      return await this.callEngines(prompt, model, updateCallback);\r\n    }\r\n  };\r\n  \r\n}\r\n\r\nexport default ApiStore;"],"mappings":"AAAA,SAASA,kBAAkB,QAAQ,MAAM;AACzC,SAASC,OAAO,EAAEC,QAAQ,EAAEC,QAAQ,EAAEC,WAAW,QAAQ,SAAS;AAClE,OAAO,KAAKC,MAAM,MAAM,QAAQ,CAAC,CAAC;AAClC,SAASC,YAAY,QAAQ,UAAU;AACvC,SAASC,GAAG,QAAQ,QAAQ;AAC5B,MAAMC,QAAQ,CAAC;EAIbC,WAAWA,CAAA,EAAG;IAAA,IAAAC,KAAA;IAAA,KAHdC,MAAM,GAAGR,QAAQ,EAAE,IAAI,EAAE;IAAA,KACzBS,aAAa,GAAGC,YAAY,CAACC,OAAO,CAAC,eAAe,CAAC,IAAI,eAAe;IAAA,KAMxEC,SAAS,GAAIJ,MAAM,IAAK;MAEtB,IAAI,CAACA,MAAM,GAAGA,MAAM;MACpBT,QAAQ,CAACS,MAAM,CAAC;IAClB,CAAC;IAAA,KAEDK,SAAS,GAAG,MAAM;MAChB,OAAO,IAAI,CAACL,MAAM;IACpB,CAAC;IAAA,KAEDM,YAAY,GAAG,MAAM;MACnB,IAAI,CAACN,MAAM,GAAG,EAAE;MAChBP,WAAW,EAAE;MACbH,OAAO,CAACiB,IAAI,CAAC,QAAQ,CAAC;IACxB,CAAC;IAAA,KAEDC,gBAAgB,GAAIC,KAAK,IAAK;MAC5B,IAAI,CAACR,aAAa,GAAGQ,KAAK;MAC1BP,YAAY,CAACQ,OAAO,CAAC,eAAe,EAAED,KAAK,CAAC;IAC9C,CAAC;IAAA,KAEDE,gBAAgB,GAAG,MAAM;MACvB,OAAO,IAAI,CAACV,aAAa;IAC3B,CAAC;IAAA,KAGDW,aAAa,GAAG,OAAOC,QAAQ,EAAEC,MAAM,EAAEL,KAAK,EAAEM,SAAS,EAAEC,cAAc,KAAK;MAC5E,IAAI;QAEF,MAAMC,WAAW,GAAG;UAClBR,KAAK,EAAEA,KAAK;UAEZS,UAAU,EAAEH,SAAS;UACrBI,WAAW,EAAE,GAAG;UAChBC,MAAM,EAAE;QACV,CAAC;QAGD,IAAIX,KAAK,KAAK,KAAK,EAAE;UACnB,OAAOQ,WAAW,CAACR,KAAK;UACxBQ,WAAW,CAACH,MAAM,GAAGA,MAAM;QAC7B,CAAC,MAAI;UACHG,WAAW,CAACI,QAAQ,GAAGP,MAAM;QAC/B;QAEA,MAAMQ,QAAQ,GAAG,MAAMC,KAAK,CAACV,QAAQ,EAAE;UACrCW,MAAM,EAAE,MAAM;UACdC,OAAO,EAAE;YACP,cAAc,EAAE,kBAAkB;YAClCC,aAAa,EAAG,UAAS,IAAI,CAAC1B,MAAO;UACvC,CAAC;UACD2B,IAAI,EAAEC,IAAI,CAACC,SAAS,CAACZ,WAAW;QAClC,CAAC,CAAC;QAGF,IAAIK,QAAQ,CAACQ,MAAM,KAAK,GAAG,EAAE;UAC3B,IAAIhB,MAAM,CAAC,CAAC,CAAC,CAACiB,OAAO,KAAK,MAAM,EAAC;YAC/BC,EAAE;UACJ;UACA,MAAMC,MAAM,GAAGX,QAAQ,CAACK,IAAI,CAACO,SAAS,EAAE;UACxC,IAAIC,MAAM,GAAG,EAAE;UAEf,MAAMC,aAAa,GAAG,MAAAA,CAAA,KAAY;YAChC,MAAM;cAAEC,IAAI;cAAEC;YAAM,CAAC,GAAG,MAAML,MAAM,CAACM,IAAI,EAAE;YAE3C,IAAIF,IAAI,EAAE;cACR;cACA,OAAOF,MAAM;YACf;YAEA,MAAMK,IAAI,GAAG,IAAIC,WAAW,CAAC,OAAO,CAAC,CAACC,MAAM,CAACJ,KAAK,CAAC;YACnD,MAAMK,KAAK,GAAGH,IAAI,aAAJA,IAAI,uBAAJA,IAAI,CAAEI,KAAK,CAAC,IAAI,CAAC,CAACC,MAAM,CAAEC,IAAI,IAAKA,IAAI,CAACC,IAAI,EAAE,KAAK,EAAE,CAAC;YAEpE,KAAK,MAAMD,IAAI,IAAIH,KAAK,EAAE;cAExB,MAAMK,OAAO,GAAGF,IAAI,CAACG,OAAO,CAAC,SAAS,EAAE,EAAE,CAAC;cAC3C,IAAID,OAAO,KAAK,QAAQ,EAAE;gBACxB,MAAMf,MAAM,CAACiB,MAAM,EAAE,CAAC,CAAC;gBACvB,MAAM,CAAC;cACT;;cACA,IAAI;gBACF,MAAMC,MAAM,GAAGvB,IAAI,CAACwB,KAAK,CAACJ,OAAO,CAAC;gBAClC,IAAIK,OAAO;gBACX,IAAI5C,KAAK,KAAK,KAAK,EAAE;kBACnB4C,OAAO,GAAGF,MAAM,CAACG,OAAO,CAAC,CAAC,CAAC,CAACd,IAAI;gBAClC,CAAC,MAAM;kBACLa,OAAO,GAAGF,MAAM,CAACG,OAAO,CAAC,CAAC,CAAC,CAACC,KAAK,CAACxB,OAAO;gBAC3C;gBAEA,IAAIsB,OAAO,KAAKG,SAAS,EAAE;kBACzBrB,MAAM,IAAIkB,OAAO;kBACjB,IAAIrC,cAAc,EAAE;oBAClBA,cAAc,CAACqC,OAAO,CAAC;kBAEzB;gBACF;gBACC;;gBAED;cAGF,CAAC,CAAC,OAAOI,KAAK,EAAE;gBACdC,OAAO,CAACD,KAAK,CAAC,qCAAqC,EAAET,OAAO,EAAES,KAAK,CAAC;cACtE;YACF;YAEA,OAAOrB,aAAa,EAAE,CAAC,CAAC;UAC1B,CAAC;;UAED,MAAMuB,WAAW,GAAG,MAAMvB,aAAa,EAAE;UACzC,OAAOtB,MAAM,KAAK,MAAM,GAAG,IAAI,GAAG6C,WAAW;QAC/C,CAAC,MAAM;UACL,MAAM,IAAIC,KAAK,CAAE,UAAStC,QAAQ,CAACQ,MAAO,IAAGR,QAAQ,CAACuC,UAAW,EAAC,CAAC;QACrE;MACF,CAAC,CAAC,OAAOJ,KAAK,EAAE;QACdC,OAAO,CAACD,KAAK,CAACA,KAAK,CAAC;QACpB,OAAO,KAAK;MACd;IACF,CAAC;IAAA,KAEDK,WAAW,GAAG,kBAAuE;MAAA,IAAhEhD,MAAM,GAAAiD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAP,SAAA,GAAAO,SAAA,MAAG,MAAM;MAAA,IAAEtD,KAAK,GAAAsD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAP,SAAA,GAAAO,SAAA,MAAG,kBAAkB;MAAA,IAAE/C,cAAc,GAAA+C,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAP,SAAA;MAC9EE,OAAO,CAACO,GAAG,CAAC,uBAAuB,EAACnD,MAAM,CAAC;MAC3C,MAAMC,SAAS,GAAGD,MAAM,KAAK,MAAM,GAAG,IAAI,GAAGnB,YAAY,CAACmB,MAAM,CAAC,GAAG,CAAC;MACrE,MAAMD,QAAQ,GAAI,qCAAoCJ,KAAM,cAAa;MAEzE,OAAO,MAAMV,KAAI,CAACa,aAAa,CAACC,QAAQ,EAAEC,MAAM,EAAE,KAAK,EAAEC,SAAS,EAAEC,cAAc,CAAC;IACrF,CAAC;IAAA,KAGDkD,OAAO,GAAG,kBAA2F;MAAA,IAApFpD,MAAM,GAAAiD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAP,SAAA,GAAAO,SAAA,MAAG,CAAC;QAAC,MAAM,EAAE,MAAM;QAAE,SAAS,EAAE;MAAM,CAAC,CAAC;MAAA,IAAEtD,KAAK,GAAAsD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAP,SAAA,GAAAO,SAAA,MAAG,OAAO;MAAA,IAAE/C,cAAc,GAAA+C,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAP,SAAA;MAC9FE,OAAO,CAACO,GAAG,CAAC,mBAAmB,EAACnD,MAAM,CAAC;MACvC,IAAIC,SAAS;MACb,IAAID,MAAM,KAAK,MAAM,EAAE;QACrBC,SAAS,GAAG,CAAC;MACf,CAAC,MAAM,IAAIN,KAAK,KAAK,OAAO,EAAE;QAC5BM,SAAS,GAAG,IAAI,GAAGpB,YAAY,CAACmB,MAAM,CAAC;MACzC,CAAC,MAAM;QACLC,SAAS,GAAG,IAAI,GAAGpB,YAAY,CAACmB,MAAM,CAAC;MACzC;MACA,MAAMD,QAAQ,GAAG,4CAA4C;MAE7D,OAAO,MAAMd,KAAI,CAACa,aAAa,CAACC,QAAQ,EAAEC,MAAM,EAAEL,KAAK,EAAEM,SAAS,EAAEC,cAAc,CAAC;IACrF,CAAC;IAAA,KAIDmD,SAAS,GAAG,kBAAiF;MAAA,IAA1ErD,MAAM,GAAAiD,SAAA,CAAAC,MAAA,QAAAD,SAAA,QAAAP,SAAA,GAAAO,SAAA,MAAG,CAAC;QAAC,MAAM,EAAE,MAAM;QAAE,SAAS,EAAE;MAAM,CAAC,CAAC;MAAA,IAAEtD,KAAK,GAAAsD,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAP,SAAA;MAAA,IAAExC,cAAc,GAAA+C,SAAA,CAAAC,MAAA,OAAAD,SAAA,MAAAP,SAAA;MACtF;MACAE,OAAO,CAACO,GAAG,CAAC,qBAAqB,EAACnD,MAAM,CAAC;MACzC,IAAIL,KAAK,KAAK,eAAe,IAAIA,KAAK,KAAK,OAAO,EAAE;QAClD,OAAO,MAAMV,KAAI,CAACmE,OAAO,CAACpD,MAAM,EAAEL,KAAK,EAAEO,cAAc,CAAC;MAC1D,CAAC,MAAM;QACL,OAAO,MAAMjB,KAAI,CAAC+D,WAAW,CAAChD,MAAM,EAAEL,KAAK,EAAEO,cAAc,CAAC;MAC9D;IACF,CAAC;IA7JC3B,kBAAkB,CAAC,IAAI,CAAC;EAC1B;AA8JF;AAEA,eAAeQ,QAAQ"},"metadata":{},"sourceType":"module","externalDependencies":[]}